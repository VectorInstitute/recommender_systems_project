{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-student",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The following notebook contains a demo of a method for sequence aware product recommendation. In particular, the [Short-term and Long-term preference Integrated\n",
    "Recommender system](https://www.microsoft.com/en-us/research/uploads/prod/2019/07/IJCAI19-ready_v1.pdf) (SLi-Rec) method is applied to the [Amazon Review Dataset](https://nijianmo.github.io/amazon/index.html). Specifically, the Movies and TV dataset is used which contains 8,765,568 reviews of 203,970 products. \n",
    "\n",
    "## Short-term and Long-term preference Integrated Recommender system (SLi-Rec)\n",
    "\n",
    "In this tutorial, we will specifically explore the SLi-Rec method. SLi-Rec jointly models long term preferences with an Asymmetric SVD and short term preferences with an RNN. The RNN is chosen to be an LSTM that is extended to include a time aware and context aware controller that enhance the state transition function to consider temporal and contextual information, respectively. The long term and short term preferences are combined dynamically based on the context. \n",
    "\n",
    "### Short Term Perferences\n",
    "\n",
    "SLi-Rec leverages an augmented LSTM to generate the short term preferences of users. In general, the LSTM is an RNN thats specified as follows: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"847\" alt=\"lstm4\" src=\"https://user-images.githubusercontent.com/34798787/179023828-c610daa2-29a8-4894-920f-6e7d1b3aa5f5.png\">\n",
    "</p>\n",
    "\n",
    "The input of an LSTM cell consists of an embedding encoding the latest user interaction $x_t$, the previous hidden state $h_{t-1}$ and the previous cell status $c_{t-1}$. The output of an LSTM cell consists of the current hidden state $h_t$ and cell status $c_t$. $i_t$, $o_t$ and $f_t$ are the input, output and forget gate, respectively. They  modulate the flow of data throughout the LSTM cell. The gates are parametized by trainiable weight matrices $U^*$ and $W^*$ as well as bias vectors $b^*$. The activation functions used in the LSTM cell include the sigmoid function $\\sigma$ and the tanh function $\\phi$. \n",
    "\n",
    "In some domains such as NLP, we can consider items in a sequence evenly spaced and semantically equivalent. However, the dynamics of user behaviour is much more complicated. As such, novel components are proposed to address the following challenges: \n",
    "\n",
    "1. **Time Irregularity:** Time intervals between actions vary.  To address time irregularity, the gate logic of the LSTM is augmented to make it sensitive time changes. \n",
    "2. **Semantic Irregularity:** Items within a users behaviour sequence may not always share the same semantic topic. To address semantic irregularity, an attention mechanism is adopted to suppress the contribution of interactions that are dissimilar to a given target item sematically. \n",
    "\n",
    "### Long Term Preferences\n",
    "The long-term preferences of a user are assumed to be static. Matrix Factorization techniques have shown strong sucess in modelling the static preferences of users. Accordingly, an Assymetric SVD is used for long term modelling. An Asymmetric SVD avoid explicitly defining representations for users and instead represents them as a weighted average of previously purchased items. Specifically, the Attention Asymmetric SVD is used as outlined in the previous notebook. \n",
    "\n",
    "### Adaptive Fusion\n",
    "To generate the final output, the prediction from the short-term and long-term model are fused based on the context using attention. For more details about this operation, or any other details regarding the method, please refer to the [paper](https://www.ijcai.org/proceedings/2019/0585.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-neighborhood",
   "metadata": {},
   "source": [
    "## Package Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "\n",
    "# Local Imports \n",
    "from utils import create_vocab\n",
    "from model import SLI_RECModel_Custom as SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "robust-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/amazon\"\n",
    "BASE_LOG_PATH = \"logs\"\n",
    "BASE_MODEL_PATH = \"models\"\n",
    "REVIEWS_FILE = 'reviews_Movies_and_TV_5.json'\n",
    "META_FILE = 'meta_Movies_and_TV.json'\n",
    "\n",
    "YAML_PATH = \"../../recommenders/recommenders/models/deeprec/config/sli_rec.yaml\"\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_num_ngs = 4\n",
    "valid_num_ngs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "steady-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "failing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories to store train, validation and test splits\n",
    "train_path = os.path.join(DATA_PATH, r'train_data')\n",
    "valid_path = os.path.join(DATA_PATH, r'valid_data')\n",
    "test_path = os.path.join(DATA_PATH, r'test_data')\n",
    "\n",
    "# Files paths to store the list of existing ids for user, item and item category \n",
    "user_vocab_path = os.path.join(DATA_PATH, r'user_vocab.pkl')\n",
    "item_vocab_path = os.path.join(DATA_PATH, r'item_vocab.pkl')\n",
    "cate_vocab_path = os.path.join(DATA_PATH, r'category_vocab.pkl')\n",
    "output_file_path = os.path.join(DATA_PATH, r'output.txt')\n",
    "\n",
    "# File paths to store reviews and associated metadata\n",
    "reviews_path = os.path.join(DATA_PATH, REVIEWS_FILE)\n",
    "meta_path = os.path.join(DATA_PATH, META_FILE)\n",
    "\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "\n",
    "# create run drectory to store results\n",
    "if os.path.exists(BASE_MODEL_PATH) == False:\n",
    "    os.mkdir(BASE_MODEL_PATH)\n",
    "    \n",
    "MODEL_PATH = f\"{BASE_MODEL_PATH}/{str(time.time())}\"\n",
    "\n",
    "if os.path.exists(BASE_LOG_PATH) == False:\n",
    "    os.mkdir(BASE_LOG_PATH)\n",
    "\n",
    "LOG_PATH = f\"{BASE_LOG_PATH}/{str(time.time())}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-learning",
   "metadata": {},
   "source": [
    "## Data Loading \n",
    "\n",
    "Given that the data is preprocessed in the [amazon_preprocessing notebook](amazon_preprocessing.ipynb), no further processing is required. In this section, we will briefly analyze the train, validation and test sets to get aquainted with the data we will be modelling. Futhermore, a data loader will be defined to iteratively fetch samples from the datasets during training and evaluation. \n",
    "\n",
    "The train dataset consists of a dataframe where each record is a review of a product `item_id` in category `cate_id` at time `timestamp` by user `user_id`. Each record also contains the list of previous items the user interacted with `prev_ids` along with the corresponding categories `prev_cate_ids` and timestamps `prev_timestamps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B008220C38</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1362441600</td>\n",
       "      <td>B005LAIHQS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1361232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365033600</td>\n",
       "      <td>B005LAIHQS,B008220C38</td>\n",
       "      <td>Movies,Movies</td>\n",
       "      <td>1361232000,1362441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00B74MJOS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1367625600</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA</td>\n",
       "      <td>Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B0067EKYL8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1371686400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS</td>\n",
       "      <td>Movies,Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>0792839072</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1372982400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>1</td>\n",
       "      <td>A1WZZDWYPVST2M</td>\n",
       "      <td>B008JFUUIA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365552000</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>1</td>\n",
       "      <td>A37K6TJ94ZFXVQ</td>\n",
       "      <td>B008JFUOWM</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1390262400</td>\n",
       "      <td>B00B74MJOS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1368144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>1</td>\n",
       "      <td>A16342W88H5YWK</td>\n",
       "      <td>B0090SI3ZW</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1364256000</td>\n",
       "      <td>B007R6D74G</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1348185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>1</td>\n",
       "      <td>AA3UZRM4EFLK2</td>\n",
       "      <td>B0067EKYL8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365465600</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>1</td>\n",
       "      <td>A1LM42U1DOCBQ</td>\n",
       "      <td>B009934S5M</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1369008000</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1362441600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16635 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label         user_id     item_id cate_id   timestamp  \\\n",
       "0          1   AWF2S3UNW9UA0  B008220C38  Movies  1362441600   \n",
       "1          1   AWF2S3UNW9UA0  B009AMANBA  Movies  1365033600   \n",
       "2          1   AWF2S3UNW9UA0  B00B74MJOS  Movies  1367625600   \n",
       "3          1   AWF2S3UNW9UA0  B0067EKYL8  Movies  1371686400   \n",
       "4          1   AWF2S3UNW9UA0  0792839072  Movies  1372982400   \n",
       "...      ...             ...         ...     ...         ...   \n",
       "16630      1  A1WZZDWYPVST2M  B008JFUUIA  Movies  1365552000   \n",
       "16631      1  A37K6TJ94ZFXVQ  B008JFUOWM  Movies  1390262400   \n",
       "16632      1  A16342W88H5YWK  B0090SI3ZW  Movies  1364256000   \n",
       "16633      1   AA3UZRM4EFLK2  B0067EKYL8  Movies  1365465600   \n",
       "16634      1   A1LM42U1DOCBQ  B009934S5M  Movies  1369008000   \n",
       "\n",
       "                                           prev_item_ids  \\\n",
       "0                                             B005LAIHQS   \n",
       "1                                  B005LAIHQS,B008220C38   \n",
       "2                       B005LAIHQS,B008220C38,B009AMANBA   \n",
       "3            B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS   \n",
       "4      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "...                                                  ...   \n",
       "16630                                         B005S9ELM6   \n",
       "16631                                         B00B74MJOS   \n",
       "16632                                         B007R6D74G   \n",
       "16633                                         B005S9ELM6   \n",
       "16634                                         B005S9ELM6   \n",
       "\n",
       "                            prev_cate_ids  \\\n",
       "0                                  Movies   \n",
       "1                           Movies,Movies   \n",
       "2                    Movies,Movies,Movies   \n",
       "3             Movies,Movies,Movies,Movies   \n",
       "4      Movies,Movies,Movies,Movies,Movies   \n",
       "...                                   ...   \n",
       "16630                              Movies   \n",
       "16631                              Movies   \n",
       "16632                              Movies   \n",
       "16633                              Movies   \n",
       "16634                              Movies   \n",
       "\n",
       "                                         prev_timestamps  \n",
       "0                                             1361232000  \n",
       "1                                  1361232000,1362441600  \n",
       "2                       1361232000,1362441600,1365033600  \n",
       "3            1361232000,1362441600,1365033600,1367625600  \n",
       "4      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "...                                                  ...  \n",
       "16630                                         1365552000  \n",
       "16631                                         1368144000  \n",
       "16632                                         1348185600  \n",
       "16633                                         1365465600  \n",
       "16634                                         1362441600  \n",
       "\n",
       "[16635 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-alias",
   "metadata": {},
   "source": [
    "The validation and test datasets share the schema as the train dataset. The only key distinction is that the evaluations sets contain negative samples which are denoted by a label of 0. Negative samples are interactions between users and items that have not occured. They are included so we can compute metrics of how well the generated recommendations approximate the users actual behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "psychological-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00005K3OT</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B0090SI3ZW</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00E8RK5OC</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>6305171769</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00005JPFX</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34360</th>\n",
       "      <td>1</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00E8RK5OC</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34361</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00005JPS8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34362</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B009934S5M</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34363</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B000E1MTYK</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34364</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00005JLXH</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34365 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label         user_id     item_id cate_id   timestamp  \\\n",
       "0          1   AWF2S3UNW9UA0  B00005K3OT  Movies  1393718400   \n",
       "1          0   AWF2S3UNW9UA0  B0090SI3ZW  Movies  1393718400   \n",
       "2          0   AWF2S3UNW9UA0  B00E8RK5OC  Movies  1393718400   \n",
       "3          0   AWF2S3UNW9UA0  6305171769  Movies  1393718400   \n",
       "4          0   AWF2S3UNW9UA0  B00005JPFX  Movies  1393718400   \n",
       "...      ...             ...         ...     ...         ...   \n",
       "34360      1  A173F44ZGP878J  B00E8RK5OC  Movies  1383264000   \n",
       "34361      0  A173F44ZGP878J  B00005JPS8  Movies  1383264000   \n",
       "34362      0  A173F44ZGP878J  B009934S5M  Movies  1383264000   \n",
       "34363      0  A173F44ZGP878J  B000E1MTYK  Movies  1383264000   \n",
       "34364      0  A173F44ZGP878J  B00005JLXH  Movies  1383264000   \n",
       "\n",
       "                                           prev_item_ids  \\\n",
       "0      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "1      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "2      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "3      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "4      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "...                                                  ...   \n",
       "34360                                         B009AMANBA   \n",
       "34361                                         B009AMANBA   \n",
       "34362                                         B009AMANBA   \n",
       "34363                                         B009AMANBA   \n",
       "34364                                         B009AMANBA   \n",
       "\n",
       "                                           prev_cate_ids  \\\n",
       "0      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "1      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "2      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "3      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "4      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "...                                                  ...   \n",
       "34360                                             Movies   \n",
       "34361                                             Movies   \n",
       "34362                                             Movies   \n",
       "34363                                             Movies   \n",
       "34364                                             Movies   \n",
       "\n",
       "                                         prev_timestamps  \n",
       "0      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "1      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "2      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "3      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "4      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "...                                                  ...  \n",
       "34360                                         1365811200  \n",
       "34361                                         1365811200  \n",
       "34362                                         1365811200  \n",
       "34363                                         1365811200  \n",
       "34364                                         1365811200  \n",
       "\n",
       "[34365 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize validation dataset dataframe\n",
    "valid_df = pd.read_csv(valid_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dependent-single",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B0000AZT3R</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B0000VD02Y</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B00005JPS8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B00003CXXO</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B000C3L27K</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169165</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B002ZG98R8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169166</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B00005JPFX</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169167</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B000AE4QD8</td>\n",
       "      <td>TV</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169168</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B000BTJDG2</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169169</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B00005JOHI</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169170 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label         user_id     item_id cate_id   timestamp prev_item_ids  \\\n",
       "0           1  A3R27T4HADWFFJ  B0000AZT3R  Movies  1389657600    B000J10EQU   \n",
       "1           0  A3R27T4HADWFFJ  B0000VD02Y  Movies  1389657600    B000J10EQU   \n",
       "2           0  A3R27T4HADWFFJ  B00005JPS8  Movies  1389657600    B000J10EQU   \n",
       "3           0  A3R27T4HADWFFJ  B00003CXXO  Movies  1389657600    B000J10EQU   \n",
       "4           0  A3R27T4HADWFFJ  B000C3L27K  Movies  1389657600    B000J10EQU   \n",
       "...       ...             ...         ...     ...         ...           ...   \n",
       "169165      0   AGAWDSE1J20RI  B002ZG98R8  Movies  1405468800    B00H7KJTCG   \n",
       "169166      0   AGAWDSE1J20RI  B00005JPFX  Movies  1405468800    B00H7KJTCG   \n",
       "169167      0   AGAWDSE1J20RI  B000AE4QD8      TV  1405468800    B00H7KJTCG   \n",
       "169168      0   AGAWDSE1J20RI  B000BTJDG2  Movies  1405468800    B00H7KJTCG   \n",
       "169169      0   AGAWDSE1J20RI  B00005JOHI  Movies  1405468800    B00H7KJTCG   \n",
       "\n",
       "       prev_cate_ids prev_timestamps  \n",
       "0             Movies      1387756800  \n",
       "1             Movies      1387756800  \n",
       "2             Movies      1387756800  \n",
       "3             Movies      1387756800  \n",
       "4             Movies      1387756800  \n",
       "...              ...             ...  \n",
       "169165        Movies      1405468800  \n",
       "169166        Movies      1405468800  \n",
       "169167        Movies      1405468800  \n",
       "169168        Movies      1405468800  \n",
       "169169        Movies      1405468800  \n",
       "\n",
       "[169170 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize test dataset dataframe\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-pixel",
   "metadata": {},
   "source": [
    "When training and evaluating neural network models, we typically feed batches of input into the model to generate predictions. This involves iterively sampling batches of data in the dataset . The [microsoft recommenders](https://github.com/microsoft/recommenders) package provides the `SequentialIterator` class which acts as a dataloader for sequential recommender systems such as SLi-Rec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-birthday",
   "metadata": {},
   "source": [
    "Following the definition of the Sequential Dataset, we need to generate the user, item and category files using the `utils.create_vocab` function. These files contain the unique ids of each user, item and category in the dataset, respectively. This is leveraged by the model to generate embedding tables of the proper dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "micro-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab(train_path, user_vocab_path, item_vocab_path, cate_vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-metropolitan",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "With the sequential dataset in place, we can turn our attention to defining the SLi-Rec model. The [microsoft recommenders](https://github.com/microsoft/recommenders) package provides a variety of recommender methods including SLi-Rec. It provides a uniform interface for both sequential and non-sequential recommender system methods. \n",
    "\n",
    "The first step to defining a model is generating the hyperameters `hparams` using the `recommenders.models.deeprec.deeprec_utils.prepare_hparams` function. This function takes a varitey of arguments including model, log save paths and vocabulary file paths along with hyperparameters such as batch size and epochs. The majority of hyperparameters are started in a yaml path at `YAML_PATH`. The microsoft recommenders package provides a [yaml file](https://github.com/microsoft/recommenders/tree/main/recommenders/models/deeprec/config) with default hyperparameters for each of the sequential architectures it supports. \n",
    "\n",
    "Some of the notable hyperparameters include: \n",
    "- **batch_size**:  The number of samples per batch\n",
    "- **epochs**: The number of times to iterate through the training set\n",
    "- **embed_l2**: The weight of the l2 regularization loss on embeddings\n",
    "- **layer_l2**: The weight of the l2 regularization loss on layer weights\n",
    "- **train_num_ngs**: The number of negative instances to include per postive instance in training set \n",
    "- **learning_rate**: Determines the step size of updates to model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powered-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(YAML_PATH, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(MODEL_PATH),\n",
    "                          SUMMARIES_DIR=os.path.join(LOG_PATH),\n",
    "                          user_vocab=user_vocab_path,\n",
    "                          item_vocab=item_vocab_path,\n",
    "                          cate_vocab=cate_vocab_path,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-proxy",
   "metadata": {},
   "source": [
    "To define the model, we pass the hyperparameters, sequential dataset and seed to the `models.SLI_RECModel_Custom` constructor. The `models.SLI_RECModel_Custom` class is a simple extension of the `recommenders.models.deeprec.models.sequential.sli_rec.SLI_RECModel` that overrides the `fit` method to more easily access train and evaluation metrics accross epochs. This is done as a convenience and is by no means required by package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "relative-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/sli_rec.py:66: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/rnn_cell_implement.py:621: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /h/jewtay/.conda/envs/recsys/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/base_model.py:705: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  training=self.is_train_stage,\n",
      "2022-08-03 15:51:50.016084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-03 15:51:50.851693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13198 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:86:00.0, compute capability: 7.5\n",
      "2022-08-03 15:51:50.946561: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-platform",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "The next step is to train and validated the model we defined using the training and validation set we previously generated using the `model.fit` method. This method trains and validates the model for a number of epochs and returns the updated model along with the train and validation results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.6109, data_loss: 1.6109\n",
      "step 40 , total_loss: 1.6089, data_loss: 1.6089\n",
      "eval valid at epoch 1: auc:0.492,logloss:0.6931,mean_mrr:0.4485,ndcg@2:0.3155,ndcg@4:0.5033,ndcg@6:0.5834,group_auc:0.4901\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_1.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_1.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_1.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.5954, data_loss: 1.5954\n",
      "step 40 , total_loss: 1.5529, data_loss: 1.5529\n",
      "eval valid at epoch 2: auc:0.5165,logloss:0.6949,mean_mrr:0.4721,ndcg@2:0.348,ndcg@4:0.5299,ndcg@6:0.6016,group_auc:0.5198\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_2.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_2.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_2.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.4373, data_loss: 1.4373\n",
      "step 40 , total_loss: 1.4727, data_loss: 1.4727\n",
      "eval valid at epoch 3: auc:0.6382,logloss:0.7601,mean_mrr:0.5925,ndcg@2:0.5196,ndcg@4:0.6589,ndcg@6:0.6938,group_auc:0.6672\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_3.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_3.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_3.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.3073, data_loss: 1.3073\n",
      "step 40 , total_loss: 1.2949, data_loss: 1.2949\n",
      "eval valid at epoch 4: auc:0.6958,logloss:0.7066,mean_mrr:0.6215,ndcg@2:0.5575,ndcg@4:0.6872,ndcg@6:0.7159,group_auc:0.6982\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_4.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_4.index\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_4.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2684, data_loss: 1.2684\n",
      "step 40 , total_loss: 1.2049, data_loss: 1.2049\n",
      "eval valid at epoch 5: auc:0.7134,logloss:0.6495,mean_mrr:0.6454,ndcg@2:0.5863,ndcg@4:0.7055,ndcg@6:0.7338,group_auc:0.7149\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_5.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_5.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_5.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2245, data_loss: 1.2245\n",
      "step 40 , total_loss: 1.1773, data_loss: 1.1773\n",
      "eval valid at epoch 6: auc:0.7294,logloss:0.6328,mean_mrr:0.6528,ndcg@2:0.5972,ndcg@4:0.7133,ndcg@6:0.7395,group_auc:0.7242\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_6.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_6.index\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_6.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2003, data_loss: 1.2003\n",
      "step 40 , total_loss: 1.2557, data_loss: 1.2557\n",
      "eval valid at epoch 7: auc:0.733,logloss:0.628,mean_mrr:0.6552,ndcg@2:0.5998,ndcg@4:0.7134,ndcg@6:0.7412,group_auc:0.7235\n",
      "step 20 , total_loss: 1.1880, data_loss: 1.1880\n",
      "step 40 , total_loss: 1.2258, data_loss: 1.2258\n",
      "eval valid at epoch 8: auc:0.7353,logloss:0.6385,mean_mrr:0.6623,ndcg@2:0.6081,ndcg@4:0.7192,ndcg@6:0.7465,group_auc:0.7286\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_8.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_8.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_8.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1330, data_loss: 1.1330\n",
      "step 40 , total_loss: 1.1742, data_loss: 1.1742\n",
      "eval valid at epoch 9: auc:0.7449,logloss:0.6446,mean_mrr:0.6683,ndcg@2:0.615,ndcg@4:0.7232,ndcg@6:0.751,group_auc:0.7334\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_9.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_9.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154epoch_9.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:models/1659556303.5191154/best_model.meta\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1344, data_loss: 1.1344\n",
      "step 40 , total_loss: 1.1907, data_loss: 1.1907\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model, train_info, eval_info = model.fit(train_path, valid_path, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack train and evaluation info\n",
    "epochs, train_metrics = zip(*train_info)\n",
    "epochs, eval_metrics = zip(*eval_info)\n",
    "\n",
    "# Reformat train metrics from list of dicts to dict of lists \n",
    "train_column_names = train_metrics[0].keys()\n",
    "train_metric_dict = {name: [] for name in train_column_names}\n",
    "for train_metric in train_metrics: \n",
    "    for key, val in train_metric.items():\n",
    "        train_metric_dict[key].append(val)\n",
    "\n",
    "# Reformat eval metrics from list of dicts to dict of lists \n",
    "eval_column_names = eval_metrics[0].keys()\n",
    "eval_metric_dict = {name: [] for name in eval_column_names}\n",
    "for eval_metric in eval_metrics:\n",
    "    for key, val in eval_metric.items():\n",
    "        eval_metric_dict[key].append(val) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-senior",
   "metadata": {},
   "source": [
    "The train metrics that are logged include: \n",
    "- **Loss**: The loss is the summation of the regularization loss and the data loss. The regularization loss penalizes the magnitude of the parameters to avoid overfitting.\n",
    "- **Data Loss:** The data loss is computed between the rating predicted by the model and the ground truth interactions. This is typically a cross entropy loss for data with implicit feedback and mean squared error loss for explicit feedback.\n",
    "\n",
    "We can visualize plot these metrics across metrics as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train metrics\n",
    "\n",
    "f, axarr = plt.subplots(len(train_metric_dict.keys()), 1, figsize=(20, 20))\n",
    "\n",
    "for i, key in enumerate(train_metric_dict.keys()): \n",
    "    index = list(range(len(train_metric_dict[key])))\n",
    "    vals = train_metric_dict[key]\n",
    "    axarr[i].set_title(f\"Train {key}\")\n",
    "    axarr[i].plot(index, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-cambodia",
   "metadata": {},
   "source": [
    "The validation metrics that are logged include: \n",
    "- **AUC**: [AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=AUC%20stands%20for%20%22Area%20under,to%20(1%2C1)) measures the likelihood that a random relevant item is ranked higher than a random irrelevant item. Higher the likelihood of this happening implies a higher AUC score meaning a better recommendation system. \n",
    "- **GROUP AUC**: Average AUC across users.\n",
    "- **Normalized discounted cumulative gain (NDCG@K)**: [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) measures the overall reward at all positions that hold a relevant item. The reward is an inverse log of the position (i.e. higher ranks for relevant items would lead to better reward, as desired).\n",
    "\n",
    "For more information regarding the aforementioned metrics, please refer to this [blog post](https://flowthytensor.medium.com/some-metrics-to-evaluate-recommendation-systems-9e0cf0c8b6cf#:~:text=AUC%20measures%20the%20likelihood%20that,meaning%20a%20better%20recommendation%20system.). \n",
    "\n",
    "We can visualize plot these metrics across metrics as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eval metrics\n",
    "\n",
    "f, axarr = plt.subplots(len(eval_metric_dict.keys()), 1, figsize=(20, 40))\n",
    "\n",
    "for i, key in enumerate(eval_metric_dict.keys()): \n",
    "    index = list(range(len(eval_metric_dict[key])))\n",
    "    vals = eval_metric_dict[key]\n",
    "    axarr[i].set_title(f\"Validation {key}\")\n",
    "    axarr[i].plot(index, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-trustee",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.run_eval(test_path, num_ngs=test_num_ngs)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results from ASVD \n",
    "asvd_res_df = pd.read_csv(\"amazon_results.csv\")\n",
    "asvd_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate SLi-rec results df \n",
    "res[\"name\"] = \"SLi-Rec\"\n",
    "res = {key: val for key, val in res.items()}\n",
    "res_df = pd.DataFrame.from_dict(res)\n",
    "res_df = res_df.set_index(\"name\")\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_res_df = pd.concat([asvd_res_df, res_df])\n",
    "amzn_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-design",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_new",
   "language": "python",
   "name": "recsys_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
