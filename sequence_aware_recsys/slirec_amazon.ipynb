{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-student",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The following notebook contains a demo of a method for sequence aware product recommendation. In particular, the [Short-term and Long-term preference Integrated\n",
    "Recommender system](https://www.microsoft.com/en-us/research/uploads/prod/2019/07/IJCAI19-ready_v1.pdf) (SLi-Rec) method is applied to the [Amazon Review Dataset](https://nijianmo.github.io/amazon/index.html). Specifically, the Movies and TV dataset is used which contains 8,765,568 reviews of 203,970 products. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-neighborhood",
   "metadata": {},
   "source": [
    "## Package Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:15:22.637682: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:22.637719: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "robust-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/amazon\"\n",
    "REVIEWS_FILE = 'reviews_Movies_and_TV_5.json'\n",
    "META_FILE = 'meta_Movies_and_TV.json'\n",
    "\n",
    "YAML_PATH = \"../../recommenders/recommenders/models/deeprec/config/sli_rec.yaml\"\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_num_ngs = 4\n",
    "valid_num_ngs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imperial-buyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjewelltaylor9430\u001b[0m (\u001b[33manomalydetection\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/h/jewtay/.local/lib/python3.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/ssd002/home/jewtay/recommender_systems_project/sequence_aware_recsys/wandb/run-20220707_141525-ce8towyi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anomalydetection/SLi-Rec/runs/ce8towyi\" target=\"_blank\">wandering-snow-1</a></strong> to <a href=\"https://wandb.ai/anomalydetection/SLi-Rec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/anomalydetection/SLi-Rec/runs/ce8towyi?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efe4681c310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='SLi-Rec', sync_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "failing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories to store train, validation and test splits\n",
    "train_path = os.path.join(DATA_PATH, r'train_data')\n",
    "valid_path = os.path.join(DATA_PATH, r'valid_data')\n",
    "test_path = os.path.join(DATA_PATH, r'test_data')\n",
    "\n",
    "# Files paths to store the list of existing ids for user, item and item category \n",
    "user_vocab_path = os.path.join(DATA_PATH, r'user_vocab.pkl')\n",
    "item_vocab_path = os.path.join(DATA_PATH, r'item_vocab.pkl')\n",
    "cate_vocab_path = os.path.join(DATA_PATH, r'category_vocab.pkl')\n",
    "output_file_path = os.path.join(DATA_PATH, r'output.txt')\n",
    "\n",
    "# File paths to store reviews and associated metadata\n",
    "reviews_path = os.path.join(DATA_PATH, REVIEWS_FILE)\n",
    "meta_path = os.path.join(DATA_PATH, META_FILE)\n",
    "\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-learning",
   "metadata": {},
   "source": [
    "## Data Loading \n",
    "\n",
    "Given that the data is preprocessed in the [amazon_preprocessing notebook](amazon_preprocessing.ipynb), no further processing is required. In this section, we will briefly analyze the train, validation and test sets to get aquainted with the data we will be modelling. Futhermore, a data loader will be defined to iteratively fetch samples from the datasets during training and evaluation. \n",
    "\n",
    "The train dataset consists of a dataframe where each record is a review of a product `item_id` in category `cate_id` at time `timestamp` by user `user_id`. Each record also contains the list of previous items the user interacted with `prev_ids` along with the corresponding categories `prev_cate_ids` and timestamps `prev_timestamps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B008220C38</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1362441600</td>\n",
       "      <td>B005LAIHQS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1361232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365033600</td>\n",
       "      <td>B005LAIHQS,B008220C38</td>\n",
       "      <td>Movies,Movies</td>\n",
       "      <td>1361232000,1362441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00B74MJOS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1367625600</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA</td>\n",
       "      <td>Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B0067EKYL8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1371686400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS</td>\n",
       "      <td>Movies,Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>0792839072</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1372982400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>1</td>\n",
       "      <td>A1WZZDWYPVST2M</td>\n",
       "      <td>B008JFUUIA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365552000</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>1</td>\n",
       "      <td>A37K6TJ94ZFXVQ</td>\n",
       "      <td>B008JFUOWM</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1390262400</td>\n",
       "      <td>B00B74MJOS</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1368144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>1</td>\n",
       "      <td>A16342W88H5YWK</td>\n",
       "      <td>B0090SI3ZW</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1364256000</td>\n",
       "      <td>B007R6D74G</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1348185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>1</td>\n",
       "      <td>AA3UZRM4EFLK2</td>\n",
       "      <td>B0067EKYL8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365465600</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>1</td>\n",
       "      <td>A1LM42U1DOCBQ</td>\n",
       "      <td>B009934S5M</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1369008000</td>\n",
       "      <td>B005S9ELM6</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1362441600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16635 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label         user_id     item_id cate_id   timestamp  \\\n",
       "0          1   AWF2S3UNW9UA0  B008220C38  Movies  1362441600   \n",
       "1          1   AWF2S3UNW9UA0  B009AMANBA  Movies  1365033600   \n",
       "2          1   AWF2S3UNW9UA0  B00B74MJOS  Movies  1367625600   \n",
       "3          1   AWF2S3UNW9UA0  B0067EKYL8  Movies  1371686400   \n",
       "4          1   AWF2S3UNW9UA0  0792839072  Movies  1372982400   \n",
       "...      ...             ...         ...     ...         ...   \n",
       "16630      1  A1WZZDWYPVST2M  B008JFUUIA  Movies  1365552000   \n",
       "16631      1  A37K6TJ94ZFXVQ  B008JFUOWM  Movies  1390262400   \n",
       "16632      1  A16342W88H5YWK  B0090SI3ZW  Movies  1364256000   \n",
       "16633      1   AA3UZRM4EFLK2  B0067EKYL8  Movies  1365465600   \n",
       "16634      1   A1LM42U1DOCBQ  B009934S5M  Movies  1369008000   \n",
       "\n",
       "                                           prev_item_ids  \\\n",
       "0                                             B005LAIHQS   \n",
       "1                                  B005LAIHQS,B008220C38   \n",
       "2                       B005LAIHQS,B008220C38,B009AMANBA   \n",
       "3            B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS   \n",
       "4      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "...                                                  ...   \n",
       "16630                                         B005S9ELM6   \n",
       "16631                                         B00B74MJOS   \n",
       "16632                                         B007R6D74G   \n",
       "16633                                         B005S9ELM6   \n",
       "16634                                         B005S9ELM6   \n",
       "\n",
       "                            prev_cate_ids  \\\n",
       "0                                  Movies   \n",
       "1                           Movies,Movies   \n",
       "2                    Movies,Movies,Movies   \n",
       "3             Movies,Movies,Movies,Movies   \n",
       "4      Movies,Movies,Movies,Movies,Movies   \n",
       "...                                   ...   \n",
       "16630                              Movies   \n",
       "16631                              Movies   \n",
       "16632                              Movies   \n",
       "16633                              Movies   \n",
       "16634                              Movies   \n",
       "\n",
       "                                         prev_timestamps  \n",
       "0                                             1361232000  \n",
       "1                                  1361232000,1362441600  \n",
       "2                       1361232000,1362441600,1365033600  \n",
       "3            1361232000,1362441600,1365033600,1367625600  \n",
       "4      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "...                                                  ...  \n",
       "16630                                         1365552000  \n",
       "16631                                         1368144000  \n",
       "16632                                         1348185600  \n",
       "16633                                         1365465600  \n",
       "16634                                         1362441600  \n",
       "\n",
       "[16635 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-alias",
   "metadata": {},
   "source": [
    "The validation and test datasets share the schema as the train dataset. The only key distinction is that the evaluations sets contain negative samples which are denoted by a label of 0. Negative samples are interactions between users and items that have not occured. They are included so we can compute metrics of how well the generated recommendations approximate the users actual behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "psychological-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00005K3OT</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B0090SI3ZW</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00E8RK5OC</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>6305171769</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>AWF2S3UNW9UA0</td>\n",
       "      <td>B00005JPFX</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1393718400</td>\n",
       "      <td>B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...</td>\n",
       "      <td>Movies,Movies,Movies,Movies,Movies,Movies,Movi...</td>\n",
       "      <td>1361232000,1362441600,1365033600,1367625600,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34360</th>\n",
       "      <td>1</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00E8RK5OC</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34361</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00005JPS8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34362</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B009934S5M</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34363</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B000E1MTYK</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34364</th>\n",
       "      <td>0</td>\n",
       "      <td>A173F44ZGP878J</td>\n",
       "      <td>B00005JLXH</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1383264000</td>\n",
       "      <td>B009AMANBA</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34365 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label         user_id     item_id cate_id   timestamp  \\\n",
       "0          1   AWF2S3UNW9UA0  B00005K3OT  Movies  1393718400   \n",
       "1          0   AWF2S3UNW9UA0  B0090SI3ZW  Movies  1393718400   \n",
       "2          0   AWF2S3UNW9UA0  B00E8RK5OC  Movies  1393718400   \n",
       "3          0   AWF2S3UNW9UA0  6305171769  Movies  1393718400   \n",
       "4          0   AWF2S3UNW9UA0  B00005JPFX  Movies  1393718400   \n",
       "...      ...             ...         ...     ...         ...   \n",
       "34360      1  A173F44ZGP878J  B00E8RK5OC  Movies  1383264000   \n",
       "34361      0  A173F44ZGP878J  B00005JPS8  Movies  1383264000   \n",
       "34362      0  A173F44ZGP878J  B009934S5M  Movies  1383264000   \n",
       "34363      0  A173F44ZGP878J  B000E1MTYK  Movies  1383264000   \n",
       "34364      0  A173F44ZGP878J  B00005JLXH  Movies  1383264000   \n",
       "\n",
       "                                           prev_item_ids  \\\n",
       "0      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "1      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "2      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "3      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "4      B005LAIHQS,B008220C38,B009AMANBA,B00B74MJOS,B0...   \n",
       "...                                                  ...   \n",
       "34360                                         B009AMANBA   \n",
       "34361                                         B009AMANBA   \n",
       "34362                                         B009AMANBA   \n",
       "34363                                         B009AMANBA   \n",
       "34364                                         B009AMANBA   \n",
       "\n",
       "                                           prev_cate_ids  \\\n",
       "0      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "1      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "2      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "3      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "4      Movies,Movies,Movies,Movies,Movies,Movies,Movi...   \n",
       "...                                                  ...   \n",
       "34360                                             Movies   \n",
       "34361                                             Movies   \n",
       "34362                                             Movies   \n",
       "34363                                             Movies   \n",
       "34364                                             Movies   \n",
       "\n",
       "                                         prev_timestamps  \n",
       "0      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "1      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "2      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "3      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "4      1361232000,1362441600,1365033600,1367625600,13...  \n",
       "...                                                  ...  \n",
       "34360                                         1365811200  \n",
       "34361                                         1365811200  \n",
       "34362                                         1365811200  \n",
       "34363                                         1365811200  \n",
       "34364                                         1365811200  \n",
       "\n",
       "[34365 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize validation dataset dataframe\n",
    "valid_df = pd.read_csv(valid_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dependent-single",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cate_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_cate_ids</th>\n",
       "      <th>prev_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B0000AZT3R</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B0000VD02Y</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B00005JPS8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B00003CXXO</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>B000C3L27K</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>B000J10EQU</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1387756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169165</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B002ZG98R8</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169166</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B00005JPFX</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169167</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B000AE4QD8</td>\n",
       "      <td>TV</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169168</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B000BTJDG2</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169169</th>\n",
       "      <td>0</td>\n",
       "      <td>AGAWDSE1J20RI</td>\n",
       "      <td>B00005JOHI</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>B00H7KJTCG</td>\n",
       "      <td>Movies</td>\n",
       "      <td>1405468800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169170 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label         user_id     item_id cate_id   timestamp prev_item_ids  \\\n",
       "0           1  A3R27T4HADWFFJ  B0000AZT3R  Movies  1389657600    B000J10EQU   \n",
       "1           0  A3R27T4HADWFFJ  B0000VD02Y  Movies  1389657600    B000J10EQU   \n",
       "2           0  A3R27T4HADWFFJ  B00005JPS8  Movies  1389657600    B000J10EQU   \n",
       "3           0  A3R27T4HADWFFJ  B00003CXXO  Movies  1389657600    B000J10EQU   \n",
       "4           0  A3R27T4HADWFFJ  B000C3L27K  Movies  1389657600    B000J10EQU   \n",
       "...       ...             ...         ...     ...         ...           ...   \n",
       "169165      0   AGAWDSE1J20RI  B002ZG98R8  Movies  1405468800    B00H7KJTCG   \n",
       "169166      0   AGAWDSE1J20RI  B00005JPFX  Movies  1405468800    B00H7KJTCG   \n",
       "169167      0   AGAWDSE1J20RI  B000AE4QD8      TV  1405468800    B00H7KJTCG   \n",
       "169168      0   AGAWDSE1J20RI  B000BTJDG2  Movies  1405468800    B00H7KJTCG   \n",
       "169169      0   AGAWDSE1J20RI  B00005JOHI  Movies  1405468800    B00H7KJTCG   \n",
       "\n",
       "       prev_cate_ids prev_timestamps  \n",
       "0             Movies      1387756800  \n",
       "1             Movies      1387756800  \n",
       "2             Movies      1387756800  \n",
       "3             Movies      1387756800  \n",
       "4             Movies      1387756800  \n",
       "...              ...             ...  \n",
       "169165        Movies      1405468800  \n",
       "169166        Movies      1405468800  \n",
       "169167        Movies      1405468800  \n",
       "169168        Movies      1405468800  \n",
       "169169        Movies      1405468800  \n",
       "\n",
       "[169170 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize test dataset dataframe\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", index_col=False, names=[\"label\", \"user_id\", \"item_id\", \"cate_id\", \"timestamp\", \"prev_item_ids\", \"prev_cate_ids\", \"prev_timestamps\"])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-pixel",
   "metadata": {},
   "source": [
    "When training and evaluating neural network models, we typically feed batches of input into the model to generate predictions. This involves iterively sampling batches of data in the dataset . The [microsoft recommenders](https://github.com/microsoft/recommenders) package provides the `SequentialIterator` class which acts as a dataloader for sequential recommender systems such as SLi-Rec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-metropolitan",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE:  \n",
    "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
    "hparams = prepare_hparams(YAML_PATH, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(DATA_PATH, \"model/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(DATA_PATH, \"summary/\"),\n",
    "                          user_vocab=user_vocab_path,\n",
    "                          item_vocab=item_vocab_path,\n",
    "                          cate_vocab=cate_vocab_path,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relative-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/sli_rec.py:66: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/rnn_cell_implement.py:621: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /h/jewtay/.conda/envs/recsys/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:15:32.474175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474445: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:32.474736: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/base_model.py:705: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  training=self.is_train_stage,\n",
      "2022-07-07 14:15:36.800484: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-07 14:15:36.803060: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-07 14:15:36.990056: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-platform",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moving-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in ../data/amazon/summary/\n",
      "2022-07-07 14:15:37.730633: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-07 14:15:37.730670: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.6076, data_loss: 1.6076\n",
      "step 40 , total_loss: 1.6033, data_loss: 1.6033\n",
      "eval valid at epoch 1: auc:0.5075,logloss:0.6958,mean_mrr:0.4637,ndcg@2:0.3357,ndcg@4:0.5214,ndcg@6:0.5952,group_auc:0.5117\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.data-00000-of-00001\n",
      "INFO:tensorflow:3100\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.index\n",
      "INFO:tensorflow:3100\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.meta\n",
      "INFO:tensorflow:5000\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.5672, data_loss: 1.5672\n",
      "step 40 , total_loss: 1.5190, data_loss: 1.5190\n",
      "eval valid at epoch 2: auc:0.5691,logloss:0.712,mean_mrr:0.5174,ndcg@2:0.4138,ndcg@4:0.5777,ndcg@6:0.6363,group_auc:0.5759\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.index\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.4522, data_loss: 1.4522\n",
      "step 40 , total_loss: 1.3732, data_loss: 1.3732\n",
      "eval valid at epoch 3: auc:0.6745,logloss:0.7773,mean_mrr:0.6018,ndcg@2:0.5311,ndcg@4:0.6683,ndcg@6:0.7009,group_auc:0.6772\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.3273, data_loss: 1.3273\n",
      "step 40 , total_loss: 1.2464, data_loss: 1.2464\n",
      "eval valid at epoch 4: auc:0.7034,logloss:0.683,mean_mrr:0.614,ndcg@2:0.5493,ndcg@4:0.6801,ndcg@6:0.7103,group_auc:0.69\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.index\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1789, data_loss: 1.1789\n",
      "step 40 , total_loss: 1.1705, data_loss: 1.1705\n",
      "eval valid at epoch 5: auc:0.7159,logloss:0.6212,mean_mrr:0.6473,ndcg@2:0.5891,ndcg@4:0.706,ndcg@6:0.7352,group_auc:0.716\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2263, data_loss: 1.2263\n",
      "step 40 , total_loss: 1.2294, data_loss: 1.2294\n",
      "eval valid at epoch 6: auc:0.7285,logloss:0.5853,mean_mrr:0.656,ndcg@2:0.5985,ndcg@4:0.7157,ndcg@6:0.7418,group_auc:0.7245\n",
      "INFO:tensorflow:../data/amazon/model/epoch_6.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_6.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_6.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2144, data_loss: 1.2144\n",
      "step 40 , total_loss: 1.1971, data_loss: 1.1971\n",
      "eval valid at epoch 7: auc:0.7343,logloss:0.6551,mean_mrr:0.6599,ndcg@2:0.6052,ndcg@4:0.7186,ndcg@6:0.7448,group_auc:0.7286\n",
      "INFO:tensorflow:../data/amazon/model/epoch_7.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_7.index\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_7.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1759, data_loss: 1.1759\n",
      "step 40 , total_loss: 1.1572, data_loss: 1.1572\n",
      "eval valid at epoch 8: auc:0.7345,logloss:0.7042,mean_mrr:0.6667,ndcg@2:0.6152,ndcg@4:0.7252,ndcg@6:0.7499,group_auc:0.7358\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1126, data_loss: 1.1126\n",
      "step 40 , total_loss: 1.2014, data_loss: 1.2014\n",
      "eval valid at epoch 9: auc:0.7379,logloss:0.6743,mean_mrr:0.6708,ndcg@2:0.6193,ndcg@4:0.7279,ndcg@6:0.753,group_auc:0.7387\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.index\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1759, data_loss: 1.1759\n",
      "step 40 , total_loss: 1.1891, data_loss: 1.1891\n",
      "eval valid at epoch 10: auc:0.7385,logloss:0.6705,mean_mrr:0.6678,ndcg@2:0.6135,ndcg@4:0.7242,ndcg@6:0.7507,group_auc:0.7348\n",
      "[(1, {'auc': 0.5075, 'logloss': 0.6958, 'mean_mrr': 0.4637, 'ndcg@2': 0.3357, 'ndcg@4': 0.5214, 'ndcg@6': 0.5952, 'group_auc': 0.5117}), (2, {'auc': 0.5691, 'logloss': 0.712, 'mean_mrr': 0.5174, 'ndcg@2': 0.4138, 'ndcg@4': 0.5777, 'ndcg@6': 0.6363, 'group_auc': 0.5759}), (3, {'auc': 0.6745, 'logloss': 0.7773, 'mean_mrr': 0.6018, 'ndcg@2': 0.5311, 'ndcg@4': 0.6683, 'ndcg@6': 0.7009, 'group_auc': 0.6772}), (4, {'auc': 0.7034, 'logloss': 0.683, 'mean_mrr': 0.614, 'ndcg@2': 0.5493, 'ndcg@4': 0.6801, 'ndcg@6': 0.7103, 'group_auc': 0.69}), (5, {'auc': 0.7159, 'logloss': 0.6212, 'mean_mrr': 0.6473, 'ndcg@2': 0.5891, 'ndcg@4': 0.706, 'ndcg@6': 0.7352, 'group_auc': 0.716}), (6, {'auc': 0.7285, 'logloss': 0.5853, 'mean_mrr': 0.656, 'ndcg@2': 0.5985, 'ndcg@4': 0.7157, 'ndcg@6': 0.7418, 'group_auc': 0.7245}), (7, {'auc': 0.7343, 'logloss': 0.6551, 'mean_mrr': 0.6599, 'ndcg@2': 0.6052, 'ndcg@4': 0.7186, 'ndcg@6': 0.7448, 'group_auc': 0.7286}), (8, {'auc': 0.7345, 'logloss': 0.7042, 'mean_mrr': 0.6667, 'ndcg@2': 0.6152, 'ndcg@4': 0.7252, 'ndcg@6': 0.7499, 'group_auc': 0.7358}), (9, {'auc': 0.7379, 'logloss': 0.6743, 'mean_mrr': 0.6708, 'ndcg@2': 0.6193, 'ndcg@4': 0.7279, 'ndcg@6': 0.753, 'group_auc': 0.7387}), (10, {'auc': 0.7385, 'logloss': 0.6705, 'mean_mrr': 0.6678, 'ndcg@2': 0.6135, 'ndcg@4': 0.7242, 'ndcg@6': 0.7507, 'group_auc': 0.7348})]\n",
      "best epoch: 9\n",
      "Time cost for training is 11.55 mins\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = model.fit(train_path, valid_path, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "starting-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data_loss</td><td>██████▇▇▆▆▅▅▄▅▄▄▄▄▃▃▃▃▃▃▂▃▂▃▂▄▂▁▃▃▂▃▂▃▂▂</td></tr><tr><td>global_step</td><td>▁▂▅▆████████████████████████████████████</td></tr><tr><td>loss</td><td>██████▇▇▆▆▅▅▄▅▄▄▄▄▃▃▃▃▃▃▂▃▂▃▂▄▂▁▃▃▂▃▂▃▂▂</td></tr><tr><td>regular_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data_loss</td><td>1.1889</td></tr><tr><td>global_step</td><td>41</td></tr><tr><td>loss</td><td>1.1889</td></tr><tr><td>regular_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wandering-snow-1</strong>: <a href=\"https://wandb.ai/anomalydetection/SLi-Rec/runs/ce8towyi\" target=\"_blank\">https://wandb.ai/anomalydetection/SLi-Rec/runs/ce8towyi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220707_141525-ce8towyi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
