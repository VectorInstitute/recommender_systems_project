{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sublime-neighborhood",
   "metadata": {},
   "source": [
    "## Package Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pretty-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing, _create_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "failing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/amazon\"\n",
    "YAML_PATH = \"../../recommenders/recommenders/models/deeprec/config/sli_rec.yaml\"\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-learning",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dressed-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_PATH): \n",
    "    os.system(f\"rm -r {DATA_PATH}\")\n",
    "\n",
    "os.mkdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emotional-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 692k/692k [00:32<00:00, 21.2kKB/s] \n",
      "100%|██████████| 97.5k/97.5k [00:07<00:00, 12.7kKB/s]\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "train_file = os.path.join(DATA_PATH, r'train_data')\n",
    "valid_file = os.path.join(DATA_PATH, r'valid_data')\n",
    "test_file = os.path.join(DATA_PATH, r'test_data')\n",
    "user_vocab = os.path.join(DATA_PATH, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(DATA_PATH, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(DATA_PATH, r'category_vocab.pkl')\n",
    "output_file = os.path.join(DATA_PATH, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(DATA_PATH, reviews_name)\n",
    "meta_file = os.path.join(DATA_PATH, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interior-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "##_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "periodic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-metropolitan",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE:  \n",
    "### remember to use `_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)` to generate the user_vocab, item_vocab and cate_vocab files, if you are using your own dataset rather than using our demo Amazon dataset.\n",
    "hparams = prepare_hparams(YAML_PATH, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(DATA_PATH, \"model/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(DATA_PATH, \"summary/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relative-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/sli_rec.py:66: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/sequential/rnn_cell_implement.py:621: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /h/jewtay/.conda/envs/recsys/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 14:37:23.966885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.966993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967136: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967277: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967351: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-30 14:37:23.967420: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/h/jewtay/.local/lib/python3.7/site-packages/recommenders/models/deeprec/models/base_model.py:705: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  training=self.is_train_stage,\n",
      "2022-06-30 14:37:28.152302: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-30 14:37:28.155135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-30 14:37:28.384690: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-platform",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moving-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.6102, data_loss: 1.6102\n",
      "step 40 , total_loss: 1.6100, data_loss: 1.6100\n",
      "eval valid at epoch 1: auc:0.5108,logloss:0.6953,mean_mrr:0.4616,ndcg@2:0.3334,ndcg@4:0.5197,ndcg@6:0.5935,group_auc:0.5083\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_1.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.5652, data_loss: 1.5652\n",
      "step 40 , total_loss: 1.5011, data_loss: 1.5011\n",
      "eval valid at epoch 2: auc:0.5589,logloss:0.6955,mean_mrr:0.4976,ndcg@2:0.3841,ndcg@4:0.555,ndcg@6:0.621,group_auc:0.5484\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_2.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.4189, data_loss: 1.4189\n",
      "step 40 , total_loss: 1.3716, data_loss: 1.3716\n",
      "eval valid at epoch 3: auc:0.6475,logloss:0.7822,mean_mrr:0.5801,ndcg@2:0.5055,ndcg@4:0.6499,ndcg@6:0.6845,group_auc:0.6564\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_3.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2777, data_loss: 1.2777\n",
      "step 40 , total_loss: 1.3197, data_loss: 1.3197\n",
      "eval valid at epoch 4: auc:0.7069,logloss:0.6909,mean_mrr:0.6393,ndcg@2:0.578,ndcg@4:0.7018,ndcg@6:0.7293,group_auc:0.7109\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_4.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2579, data_loss: 1.2579\n",
      "step 40 , total_loss: 1.2608, data_loss: 1.2608\n",
      "eval valid at epoch 5: auc:0.7199,logloss:0.6603,mean_mrr:0.6536,ndcg@2:0.5983,ndcg@4:0.7141,ndcg@6:0.7401,group_auc:0.7244\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_5.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.2098, data_loss: 1.2098\n",
      "step 40 , total_loss: 1.2916, data_loss: 1.2916\n",
      "eval valid at epoch 6: auc:0.7318,logloss:0.6793,mean_mrr:0.6538,ndcg@2:0.599,ndcg@4:0.7133,ndcg@6:0.7402,group_auc:0.7242\n",
      "step 20 , total_loss: 1.2335, data_loss: 1.2335\n",
      "step 40 , total_loss: 1.2485, data_loss: 1.2485\n",
      "eval valid at epoch 7: auc:0.733,logloss:0.6218,mean_mrr:0.6527,ndcg@2:0.5982,ndcg@4:0.712,ndcg@6:0.7393,group_auc:0.7216\n",
      "step 20 , total_loss: 1.2678, data_loss: 1.2678\n",
      "step 40 , total_loss: 1.1734, data_loss: 1.1734\n",
      "eval valid at epoch 8: auc:0.7355,logloss:0.6834,mean_mrr:0.6604,ndcg@2:0.6041,ndcg@4:0.7181,ndcg@6:0.7451,group_auc:0.7271\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_8.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1890, data_loss: 1.1890\n",
      "step 40 , total_loss: 1.2215, data_loss: 1.2215\n",
      "eval valid at epoch 9: auc:0.741,logloss:0.5891,mean_mrr:0.6635,ndcg@2:0.6114,ndcg@4:0.7203,ndcg@6:0.7475,group_auc:0.731\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/epoch_9.index\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "step 20 , total_loss: 1.1098, data_loss: 1.1098\n",
      "step 40 , total_loss: 1.1646, data_loss: 1.1646\n",
      "eval valid at epoch 10: auc:0.7455,logloss:0.6726,mean_mrr:0.669,ndcg@2:0.6165,ndcg@4:0.726,ndcg@6:0.7516,group_auc:0.7359\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.index\n",
      "INFO:tensorflow:0\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.data-00000-of-00001\n",
      "INFO:tensorflow:600\n",
      "INFO:tensorflow:../data/amazon/model/epoch_10.meta\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.meta\n",
      "INFO:tensorflow:1900\n",
      "INFO:tensorflow:../data/amazon/model/best_model.data-00000-of-00001\n",
      "INFO:tensorflow:2500\n",
      "INFO:tensorflow:../data/amazon/model/best_model.index\n",
      "INFO:tensorflow:2500\n",
      "[(1, {'auc': 0.5108, 'logloss': 0.6953, 'mean_mrr': 0.4616, 'ndcg@2': 0.3334, 'ndcg@4': 0.5197, 'ndcg@6': 0.5935, 'group_auc': 0.5083}), (2, {'auc': 0.5589, 'logloss': 0.6955, 'mean_mrr': 0.4976, 'ndcg@2': 0.3841, 'ndcg@4': 0.555, 'ndcg@6': 0.621, 'group_auc': 0.5484}), (3, {'auc': 0.6475, 'logloss': 0.7822, 'mean_mrr': 0.5801, 'ndcg@2': 0.5055, 'ndcg@4': 0.6499, 'ndcg@6': 0.6845, 'group_auc': 0.6564}), (4, {'auc': 0.7069, 'logloss': 0.6909, 'mean_mrr': 0.6393, 'ndcg@2': 0.578, 'ndcg@4': 0.7018, 'ndcg@6': 0.7293, 'group_auc': 0.7109}), (5, {'auc': 0.7199, 'logloss': 0.6603, 'mean_mrr': 0.6536, 'ndcg@2': 0.5983, 'ndcg@4': 0.7141, 'ndcg@6': 0.7401, 'group_auc': 0.7244}), (6, {'auc': 0.7318, 'logloss': 0.6793, 'mean_mrr': 0.6538, 'ndcg@2': 0.599, 'ndcg@4': 0.7133, 'ndcg@6': 0.7402, 'group_auc': 0.7242}), (7, {'auc': 0.733, 'logloss': 0.6218, 'mean_mrr': 0.6527, 'ndcg@2': 0.5982, 'ndcg@4': 0.712, 'ndcg@6': 0.7393, 'group_auc': 0.7216}), (8, {'auc': 0.7355, 'logloss': 0.6834, 'mean_mrr': 0.6604, 'ndcg@2': 0.6041, 'ndcg@4': 0.7181, 'ndcg@6': 0.7451, 'group_auc': 0.7271}), (9, {'auc': 0.741, 'logloss': 0.5891, 'mean_mrr': 0.6635, 'ndcg@2': 0.6114, 'ndcg@4': 0.7203, 'ndcg@6': 0.7475, 'group_auc': 0.731}), (10, {'auc': 0.7455, 'logloss': 0.6726, 'mean_mrr': 0.669, 'ndcg@2': 0.6165, 'ndcg@4': 0.726, 'ndcg@6': 0.7516, 'group_auc': 0.7359})]\n",
      "best epoch: 10\n",
      "Time cost for training is 12.20 mins\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-cedar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
