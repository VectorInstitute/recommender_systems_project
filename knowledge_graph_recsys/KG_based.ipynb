{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook tries to develop a basic understanding of the workflow of KG-enhanved recommendation using [Pykeen](https://pykeen.readthedocs.io/).\n",
        "In the beginning, we import the necessary packages."
      ],
      "metadata": {
        "id": "HDQ3aTwmU6qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re, pickle, torch\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "from pykeen.pipeline import pipeline\n",
        "from pykeen.models import predict\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pykeen.triples import TriplesFactory\n",
        "import pickle\n",
        "import cupy as cp\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys, os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Rdo-iq2xX-Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we import the KG and recommendation data which are in separate files. Then, we form the recommendation dataset in the <user, likes, item> format and the \"likes\" relation is assigned to be the relation number \"0\" in our \"user-item\" graph. Thus, we shift the KG relations by one (not to begin from 0)."
      ],
      "metadata": {
        "id": "lSNCo6r5WQ0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "kg_path = 'datasets/www_data/www_data/Movielens/kg/train.dat'\n",
        "rec_path = 'datasets/www_data/www_data/Movielens/rs/ratings.txt'\n",
        "kg = np.genfromtxt(kg_path, delimiter='\\t', dtype=np.int32)\n",
        "rec = np.genfromtxt(rec_path, delimiter='\\t', dtype=np.int32)\n",
        "rec = rec[:,:3] # remove time col.\n",
        "rec[:,2] = rec[:,2] >= 4 # binary ratings, 0 if [0, 4), 1 if [4, 5] \n",
        "rec = rec[rec[:,2] == 1] # select only positive ratings\n",
        "rec[:,2] = 0 # set redundant col to relationship 0\n",
        "kg[:,1] += 1 # offset\n",
        "kg = remove_rare(kg) #remove rare relations\n",
        "#kg_train, kg_test = split_kg(kg)\n",
        "rec = rec[:, [0,2,1]] # <user, likes, item> format"
      ],
      "metadata": {
        "id": "F3D-2OBjV-lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove kg rels with very low frequency\n",
        "def remove_rare(kg):\n",
        "    _ , counts = np.unique(kg[:,1], return_counts=True)\n",
        "    #finding rels that occur less than 100 times\n",
        "    rare_rels = np.where(counts<100)\n",
        "    for rare_rel in rare_rels[0]:\n",
        "        kg = np.delete(kg,np.where(kg[:,1] == (rare_rel + 1)),axis=0)\n",
        "    return kg"
      ],
      "metadata": {
        "id": "ImKti1IMUC01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity matching is a crucial step in data processing for KG-enhanced recommendation. In this step, we use the entity matchings provided to mach the Freebase html codes to Movielens ids."
      ],
      "metadata": {
        "id": "a8TuOY9tW_vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOTAL_FB_IDS = np.max(kg) # total number of default kg pairs (# rel << # entities)\n",
        "# paths for converting data\n",
        "item2kg_path = 'datasets/www_data/www_data/Movielens/rs/i2kg_map.tsv'\n",
        "emap_path = 'datasets/www_data/www_data/Movielens/kg/e_map.dat'\n",
        "# maps movie lense id's to free base html links\n",
        "ml2fb_map = {}\n",
        "with open(item2kg_path) as f:\n",
        "    for line in f:\n",
        "        ml_id = re.search('(.+?)\\t', line)\n",
        "        fb_http = re.search('\\t(.+?)\\n', line)\n",
        "        ml2fb_map.update({int(ml_id.group(1)) : fb_http.group(1)})\n",
        "# maps free base html links to free base id's (final format)\n",
        "id2html_map = {}\n",
        "fb2id_map = {}\n",
        "with open(emap_path) as f:\n",
        "    for kg_id, line in enumerate(f):\n",
        "        fb_http = re.search('\\t(.+?)\\n', line)\n",
        "        fb2id_map.update({fb_http.group(1) : kg_id})\n",
        "        id2html_map.update({kg_id : fb_http.group(1)})\n",
        "# convert movielens id's to freebase id's\n",
        "i = 0\n",
        "while True:\n",
        "    if i == rec.shape[0]:\n",
        "        break\n",
        "    if rec[i,2] in ml2fb_map: \n",
        "        # get correct freebase id from data\n",
        "        fb_http = ml2fb_map[rec[i,2]]\n",
        "        fb_id = fb2id_map[fb_http]\n",
        "        rec[i,2] = fb_id\n",
        "        i += 1\n",
        "    # remove from rec (only use movies that are in kg)\n",
        "    else:\n",
        "        rec = np.delete(rec, i, axis=0)"
      ],
      "metadata": {
        "id": "rS9qvTptWFG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we match the user ids with our KG entity codes"
      ],
      "metadata": {
        "id": "QhSK2aB2XXE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_path = 'datasets/www_data/www_data/Movielens/rs/u_map.dat'\n",
        "userid2fbid_map = {}\n",
        "new_ids = 0\n",
        "with open(umap_path) as f:\n",
        "    for line in f:\n",
        "        ml_id = re.search('\\t(.+?)\\n', line)\n",
        "        if int(ml_id.group(1)) in rec[:,0]:\n",
        "            new_ids += 1\n",
        "            userid2fbid_map.update({int(ml_id.group(1)) : TOTAL_FB_IDS + new_ids})\n",
        "# convert movielens user id's into freebase id's\n",
        "for i in range(rec.shape[0]):\n",
        "    rec[i,0] = userid2fbid_map[rec[i,0]]\n",
        "NEW_USER_IDS = new_ids\n"
      ],
      "metadata": {
        "id": "xb1kOnHiXWwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the recommendation and KG data into train, test and validation. Also, we should remove items and users in the test/validation datasets that are not included in the training set (we can't expect the model to predict items/users that it hasn't been trained on. "
      ],
      "metadata": {
        "id": "CwQNRfnaX3KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data randomly to train, tets, and val sets\n",
        "def split(rec, split_test = 0.2, split_val = 0.2):\n",
        "    np.random.shuffle(rec)\n",
        "    test_start = int((1-(split_test + split_val))*rec.shape[0])\n",
        "    val_start = int((1-(split_val))*rec.shape[0])\n",
        "    rec_train = rec[:test_start]\n",
        "    rec_test = rec[test_start:val_start]\n",
        "    rec_val = rec[val_start:]\n",
        "    return rec_train , rec_test, rec_val"
      ],
      "metadata": {
        "id": "tAD7k4RSYSMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove items from test and val that aren't in train\n",
        "def remove_new(rec_test, rec_train):\n",
        "    train_items = np.unique(rec_train[:,2])\n",
        "    test_items = np.unique(rec_test[:,2])\n",
        "    invalid_items = [item for item in test_items if item not in train_items]\n",
        "    for invalid_item in invalid_items:\n",
        "      rec_test = np.delete(rec_test, np.where(rec_test[:,2]== invalid_item),axis=0)\n",
        "    train_users = np.unique(rec_train[:,0])\n",
        "    test_users = np.unique(rec_test[:,0])\n",
        "    invalid_users = [user for user in test_users if user not in train_users]\n",
        "    for invalid_user in invalid_users:\n",
        "      rec_test = np.delete(rec_test,np.where(rec_test[:,0]== invalid_user),axis=0)\n",
        "    return rec_test\n"
      ],
      "metadata": {
        "id": "G7C-b7_BYg3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_train, rec_test, rec_val = split(rec)\n",
        "rec_test = remove_new(rec_test, rec_train)\n",
        "rec_val = remove_new(rec_val, rec_train)\n",
        "\n",
        "kg_train, kg_test, kg_val = split(kg)\n",
        "\n",
        "train = np.concatenate((rec_train, kg_train))\n",
        "test = np.concatenate((rec_test, kg_test))\n",
        "val = np.concatenate((rec_val, kg_val))\n",
        "\n",
        "np.savetxt(\"train.tsv\", train,fmt=\"%1d\", delimiter=\"\\t\")\n",
        "np.savetxt(\"test.tsv\", test,fmt=\"%1d\", delimiter=\"\\t\")\n",
        "np.savetxt(\"val.tsv\", val,fmt=\"%1d\", delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "w2TeGnRIXy6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to have an efficient testing, we make a dictionary with users as the keys and the items they like in each of the train, test, and validation sets as the values and save them. "
      ],
      "metadata": {
        "id": "FUvXlun5Y1Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user likes for testing recommendation\n",
        "def user_likes(test, train):\n",
        "    tvt = (test, train)\n",
        "\n",
        "    ul = []\n",
        "    for data in tvt:\n",
        "        user_likes = {}\n",
        "        for i in range(data.shape[0]):\n",
        "            if data[i,0] not in user_likes:\n",
        "                user_likes.update({data[i,0]: [data[i,2]]})\n",
        "            else:\n",
        "                if data[i,2] not in user_likes[data[i,0]]:\n",
        "                    user_likes[data[i,0]].append(data[i,2])\n",
        "        ul.append(user_likes)\n",
        "\n",
        "    return (ul[0], ul[1]) \n"
      ],
      "metadata": {
        "id": "uRfdsLlOYqZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ul_train, ul_test = user_likes(rec_train, rec_test)\n",
        "\n",
        "\n",
        "with open('ul_train.pkl', 'wb') as f:\n",
        "    pickle.dump(ul_train,f)\n",
        "with open('ul_test.pkl', 'wb') as f:\n",
        "    pickle.dump(ul_test,f)\n",
        "with open('ul_val.pkl', 'wb') as f:\n",
        "    pickle.dump(ul_val,f)"
      ],
      "metadata": {
        "id": "JIf0CL-ZXy0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/home/admin/Desktop/Empirical/ul_train.pkl','rb') as f:\n",
        "    ul_train = pickle.load(f)\n",
        "with open('/home/admin/Desktop/Empirical/ul_test.pkl','rb') as f:\n",
        "    ul_test = pickle.load(f)\n",
        "with open('/home/admin/Desktop/Empirical/ul_val.pkl','rb') as f:\n",
        "    ul_val = pickle.load(f)"
      ],
      "metadata": {
        "id": "GYmGIR-cXysp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing using Pykeen library. Once your datasets are ready, Pykeen makes training KG models easy (the following lines). However, since the Pykeen tests all triplest (from all relations), but we are only interested on evaluating the recommendation performance of the system (only the \"likes\" relation), we implement our own testing."
      ],
      "metadata": {
        "id": "CAESzLXYZXO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func(config):\n",
        "\n",
        "    hit1s_all=[]\n",
        "    hit3s_all=[]\n",
        "    hit10s_all=[]\n",
        "    checkpoint_freq = 1\n",
        "    # in order to train a KG embedding model with Pykeen, you just need to make a \"pipeline\" like this.\n",
        "    #You specify the hyperparameters, datasets, optimizer, embedding model, and the rest is done by Pykeen\n",
        "    result = pipeline(\n",
        "            training='/home/admin/Desktop/Empirical/train.tsv',\n",
        "            testing='/home/admin/Desktop/Empirical/test.tsv',\n",
        "            #validation='/home/admin/Desktop/Empirical/val.tsv',\n",
        "            model='TransE',\n",
        "            model_kwargs=dict(embedding_dim=config[\"embedding_dim\"]),\n",
        "            optimizer='Adam',\n",
        "            optimizer_kwargs=dict(lr=config[\"lr\"]),\n",
        "            training_kwargs=dict(num_epochs=config[\"epoch\"], batch_size=config[\"batch_size\"],\n",
        "            checkpoint_name='my_checkpoint.pt',\n",
        "            checkpoint_frequency=checkpoint_freq),\n",
        "            regularizer_kwargs=dict(weight=config[\"reg_lambda\"]),\n",
        "            negative_sampler_kwargs=dict(num_negs_per_pos=config[\"neg_ratio\"]),\n",
        "                #num_negs_per_pos=dict(type=int, low=10, high=100, step=10, log=True),\n",
        "            )\n",
        "\n",
        "    result.save_to_directory('transe2')\n",
        "    #testing performance of the trained model\n",
        "    ranks=[]\n",
        "    all_items = np.unique(rec_train[:,2])\n",
        "\n",
        "    for user in tqdm(list(ul_test.keys())[0:1000]):\n",
        "      # This is how you can ask Pykeen to make predictions of the head of relation \"0\" for this specific user\n",
        "      # the output is the unsorted scores for entities\n",
        "        predicted_tails_df = predict.get_tail_prediction_df(\n",
        "            result.model, str(user), '0', triples_factory=result.training)\n",
        "      # we sort tails based on their scores and only keep \"items\" (the ones we want to recommend to user)\n",
        "        tails_sorted = predicted_tails_df['tail_id']\n",
        "        items_sorted = np.array(tails_sorted[tails_sorted.isin(all_items)])\n",
        "\n",
        "      # we require to know what are all items that the user likes for filtering correct ones when testing each ground truth\n",
        "        liked_items_all = ul_train[user]\n",
        "        try:\n",
        "            liked_items_all += ul_test[user]\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "            liked_items_all += ul_val[user]\n",
        "        except:\n",
        "            pass\n",
        "        whole_liked_items = np.array(liked_items_all)\n",
        "        whole_ranks = [(np.where(items_sorted==item))[0][0] for item in whole_liked_items]\n",
        "        # filter other correct items and calculate the ranks\n",
        "        for gt in ul_test[user]:\n",
        "            whole_index = np.where(whole_liked_items==gt)[0][0]\n",
        "            unfiltered_rank = whole_ranks[whole_index]\n",
        "            higher_others = np.where(whole_ranks<unfiltered_rank)[0].shape[0]\n",
        "            filtered_rank = unfiltered_rank-higher_others\n",
        "            ranks.append(filtered_rank)\n",
        "\n",
        "    #calculating the metrics\n",
        "    ranksarray=np.array(ranks)\n",
        "    hits1=((ranksarray<2).sum())/len(ranks)\n",
        "    hit1s_all.append(hits1)\n",
        "    hits3=((ranksarray<4).sum())/len(ranks)\n",
        "    hit3s_all.append(hits3)\n",
        "    hits10=((ranksarray<11).sum())/len(ranks)\n",
        "    hit10s_all.append(hits10)\n",
        "    print(\"hit1s:\",hit1s_all)\n",
        "    print(\"hit3s:\",hit3s_all)\n",
        "    print(\"hit10s:\",hit10s_all)\n",
        "\n"
      ],
      "metadata": {
        "id": "3QnHehn7WFD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing in hyperparameters, training, and testing"
      ],
      "metadata": {
        "id": "WHfuAvVqZ9ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config={\n",
        "    \"embedding_dim\" : 64,\n",
        "    \"lr\": 0.0291,\n",
        "    \"batch_size\" : 256,\n",
        "    \"reg_lambda\" : 0.2,\n",
        "    \"neg_ratio\" : 10,\n",
        "    \"epoch\":1\n",
        "    }\n",
        "train_func(config)"
      ],
      "metadata": {
        "id": "h37pS31bWBlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_P7RJPKWBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPDbhJrVV-iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fm-T9TQdV-aL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}