{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction\n",
    "- Introduction to Session Based Recommender Systems\n",
    "- Introduction to Method\n",
    "- Code\n",
    "    - Data\n",
    "        - Data Loading\n",
    "        - Data Preprocessing\n",
    "        - Splitting data into train, validation and test\n",
    "        - Data Loader\n",
    "    - Model\n",
    "         - Model Definition\n",
    "         - Hyperparameter Settings\n",
    "         - Loss Function\n",
    "         - Metrics for Method\n",
    "    - Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, recommendation systems are ubiquitous, spanning sectors as diverse as online retail, music and video streaming, and content publishing. Through these systems, we can discover what's interesting or important to us among the vast amount of content on the internet. A recommendation system can assist us in navigating more efficiently and making better decisions when it is implemented correctly.\n",
    "\n",
    "\n",
    "There are three types of recommendation systems: content-based, collaborative filtering-based, and context-aware methods. User preferences for product features can be identified either through previous actions or through explicit feedback, leading to content-based filtering recommendations. Alternatively, collaborative filtering refers to the use of interactions between users and items across a number of users to produce recommendations for one particular user based on the preferences of others. Generally, these systems learn users' long-term preferences based on their historical interactions with items (e.g., what they clicked on in the past). Using context-aware methods, information related to context, such as location and time, is leveraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Session Based Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost always the case that choices have a time-sensitive context; for example, some items may be more relevant than others based on when they were last viewed or purchased. Despite being embedded in the user's most recent interactions, short-term preferences may only represent a small portion of history. Additionally, a user's preference for certain items can be dynamic rather than static; it can evolve over time. As a result, session-based recommendation algorithms have been developed, which rely heavily on the user's recent interactions instead of their historical preferences. Moreover, this approach is especially advantageous since a user may appear anonymously if they are not logged in or browsing incognito.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real-life recommender systems often have to make recommendations based on short session data (for example, on a small sportsware website) instead of long user histories (like Netflix). A lot of work in recommender systems relies on models that can be built when a user identifier is available. This setting has been dominated by matrix factorization methods in the literature. Among the main approaches employed in the session-based recommendation is the item-to-item recommendation approach, which is a natural solution to the problem of a missing user profile. From the session data, an item-to-item similarity matrix is precomputed, meaning items that are often clicked together in sessions are deemed similar. The similarity matrix is used during the session to recommend items most similar to the one the user is currently viewing. Although simple, this method has proven to be effective and is widely used. Although effective, these methods only consider the last click of the user, ignoring past clicks in effect. In practice, it is argued that more accurate recommendations can be provided by modeling the whole session. A session-based recommendation system based on RNNs is therefore proposed in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:33:57.876078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:33:57.876140: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YOOCHOOSE contains a collection of sessions from a retailer, where each session contains the click events that the user performed. There are also buy events for some sessions. This means that the user purchased something from the web shop at the end of the session. Data was collected from the clicks and purchases made by users of an online retailer in Europe over several months in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ORIGINAL_DATA = \"/h/vkhazaie/yoochoose-data/\"\n",
    "PATH_TO_PROCESSED_DATA = \"/h/vkhazaie/yoochoose-data/preprocessed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PATH_TO_PROCESSED_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH_TO_ORIGINAL_DATA + 'yoochoose-clicks.dat', sep=',', header=None, usecols=[0,1,2], dtype={0:np.int32, 1:str, 2:np.int64})\n",
    "data.columns = ['SessionId', 'TimeStr', 'ItemId']\n",
    "data['Time'] = data.TimeStr.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp()) #This is not UTC. It does not really matter.\n",
    "del(data['TimeStr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>1].index)]\n",
    "item_supports = data.groupby('ItemId').size()\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports>=5].index)]\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>=2].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 31637239\n",
      "\tSessions: 7966257\n",
      "\tItems: 37483\n",
      "Test set\n",
      "\tEvents: 71222\n",
      "\tSessions: 15324\n",
      "\tItems: 6751\n",
      "Train set\n",
      "\tEvents: 31579006\n",
      "\tSessions: 7953885\n",
      "\tItems: 37483\n",
      "Validation set\n",
      "\tEvents: 58233\n",
      "\tSessions: 12372\n",
      "\tItems: 6359\n"
     ]
    }
   ],
   "source": [
    "tmax = data.Time.max()\n",
    "session_max_times = data.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < tmax-86400].index\n",
    "session_test = session_max_times[session_max_times >= tmax-86400].index\n",
    "train = data[np.in1d(data.SessionId, session_train)]\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n",
    "\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)]\n",
    "tslength = test.groupby('SessionId').size()\n",
    "test = test[np.in1d(test.SessionId, tslength[tslength>=2].index)]\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n",
    "print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n",
    "\n",
    "\n",
    "tmax = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < tmax-86400].index\n",
    "session_valid = session_max_times[session_max_times >= tmax-86400].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "tslength = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, tslength[tslength>=2].index)]\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "print('Train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train_tr), train_tr.SessionId.nunique(), train_tr.ItemId.nunique()))\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)\n",
    "print('Validation set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(valid), valid.SessionId.nunique(), valid.ItemId.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "        # Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        # clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\"\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "\n",
    "\n",
    "class SessionDataLoader:\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices (for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                inp = idx_input\n",
    "                target = idx_target\n",
    "                yield inp, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses GRU-based RNNs for session-based recommendations. The input of the network is the actual state of the session, while the output is the item of the next event. The session state can either be the item of the actual event or the events in the session so far. One-of-N encoding is used in the former case, i.e. the input vector's length equals the number of items, and only the coordinate corresponding to the active item is one, the others are zeros. In the latter setting, events that occurred earlier are discounted if they have occurred more recently. In order to ensure stability, the input vector is normalized. The GRU layer(s) is the core of the network, and additional feedforward layers can be added between it and the output. In this case, the output is a predicted preference, i.e. the likelihood of each item being next in the session. The hidden state of the previous layer becomes the input for the next layer when multiple GRU layers are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/demo1_model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(emb_size, hidden_units, batch_size, train_n_items):\n",
    "    inputs = Input(batch_shape=(batch_size, 1, train_n_items))\n",
    "    gru, gru_states = GRU(hidden_units, stateful=True, return_state=True, name=\"GRU\")(inputs)\n",
    "    drop2 = Dropout(0.25)(gru)\n",
    "    predictions = Dense(train_n_items, activation='softmax')(drop2)\n",
    "    model = Model(inputs=inputs, outputs=[predictions])\n",
    "    checkpoint = ModelCheckpoint('./model_checkpoint.h5', monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 50\n",
    "batch_size = 512\n",
    "hidden_units = 100\n",
    "train_n_items = len(train['ItemId'].unique()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:49:54.410152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.410747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.411155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.411574: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.411945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.412297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.412668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.413009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-08-02 10:49:54.413038: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-08-02 10:49:54.414802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(emb_size, hidden_units, batch_size, train_n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./GRU4REC_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/vkhazaie/.local/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(512, 1, 37484)]         0         \n",
      "                                                                 \n",
      " GRU (GRU)                   [(512, 100),              11275800  \n",
      "                              (512, 100)]                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (512, 100)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (512, 37484)              3785884   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,061,684\n",
      "Trainable params: 15,061,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the method with the following metrics:\n",
    "\n",
    "- Recall at K (Recall@K): It is the proportion of test cases in which the ground truth item is among the top K recommendations (a test example receives a score of 1 when the nth item appears, and 0 otherwise).\n",
    "\n",
    "- Mean Reciprocal Rank at K (MRR@K): For all test cases, the average reciprocal rank of the ground truth items within the top K recommendations is taken (for example, if the nth item was second in the list, its reciprocal rank would be 1/2). As a result of this metric, higher rankings are preferred in the ordered list of recommendation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test, train_generator_map, recall_k=20, mrr_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test, itemmap=train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    n = 0\n",
    "    rec_sum = 0\n",
    "    mrr_sum = 0\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    for feat, label, mask in test_generator:\n",
    "\n",
    "        gru_layer = model.get_layer(name=\"GRU\")\n",
    "        hidden_states = gru_layer.states[0].numpy()\n",
    "        for elt in mask:\n",
    "            hidden_states[elt, :] = 0\n",
    "        gru_layer.reset_states(states=hidden_states)\n",
    "\n",
    "        target_oh = to_categorical(label, num_classes=train_n_items)\n",
    "        input_oh  = to_categorical(feat,  num_classes=train_n_items)\n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx]\n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            rec_idx =  pred_row.argsort()[-recall_k:][::-1]\n",
    "            mrr_idx =  pred_row.argsort()[-mrr_k:][::-1]\n",
    "            tru_idx = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            if tru_idx[0] in rec_idx:\n",
    "                rec_sum += 1\n",
    "\n",
    "            if tru_idx[0] in mrr_idx:\n",
    "                mrr_sum += 1/int((np.where(mrr_idx == tru_idx[0])[0]+1))\n",
    "\n",
    "    recall = rec_sum/n\n",
    "    mrr = mrr_sum/n\n",
    "    return (recall, recall_k), (mrr, mrr_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_n_items = len(train['ItemId'].unique()) + 1\n",
    "train_samples_qty = len(train['SessionId'].unique()) + 1\n",
    "test_samples_qty = len(test['SessionId'].unique()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 4.26792:  28%|██▊       | 2241347/7966258 [3:47:37<9:10:50, 173.22it/s] "
     ]
    }
   ],
   "source": [
    "train_dataset = SessionDataset(train)\n",
    "\n",
    "for epoch in range(2, epochs + 1):\n",
    "    with tqdm(total=train_samples_qty) as pbar:\n",
    "        loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "        for feat, target, mask in loader:\n",
    "\n",
    "            gru_layer = model.get_layer(name=\"GRU\")\n",
    "            hidden_states = gru_layer.states[0].numpy()\n",
    "            for elt in mask:\n",
    "                hidden_states[elt, :] = 0\n",
    "            gru_layer.reset_states(states=hidden_states)\n",
    "\n",
    "            input_oh = to_categorical(feat, num_classes=loader.n_items)\n",
    "            input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "            target_oh = to_categorical(target, num_classes=loader.n_items)\n",
    "\n",
    "            tr_loss = model.train_on_batch(input_oh, target_oh)\n",
    "\n",
    "            pbar.set_description(\"Epoch {0}. Loss: {1:.5f}\".format(epoch, tr_loss))\n",
    "            pbar.update(loader.done_sessions_counter)\n",
    "\n",
    "    print(\"Saving weights...\")\n",
    "    model.save('./GRU4REC_{}.h5'.format(epoch))\n",
    "\n",
    "    (rec, rec_k), (mrr, mrr_k) = get_metrics(model, test, train_dataset.itemmap, recall_k=20, mrr_k=20)\n",
    "    print(\"\\t - Recall@{} epoch {}: {:5f}\".format(rec_k, epoch, rec))\n",
    "    print(\"\\t - MRR@{}    epoch {}: {:5f}\\n\".format(mrr_k, epoch, mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\t - Recall@20 epoch -1: 0.695763\n",
      "\t - MRR@20    epoch -1: 0.299096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(rec, rec_k), (mrr, mrr_k) = get_metrics(model, test, train_dataset.itemmap, recall_k=20, mrr_k=20)\n",
    "print(\"\\t - Recall@{} epoch {}: {:5f}\".format(rec_k, -1, rec))\n",
    "print(\"\\t - MRR@{}    epoch {}: {:5f}\\n\".format(mrr_k, -1, mrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://arxiv.org/pdf/1511.06939.pdf\" title=\"Session-based recommendations with recurrent neural networks\">Hidasi, Balázs, et al. Session-based recommendations with recurrent neural networks.</a>\n",
    "2. https://github.com/paxcema/KerasGRU4Rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
